{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741b8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Harmonische Reihe\n",
    "# \n",
    "\n",
    "#####################################################\n",
    "# L√∂sung Studierende\n",
    "#####################################################\n",
    "\n",
    "# BEGIN Vorbelegung des Antwortfeldes\n",
    "def harmonic_series(n:int):\n",
    "    # Ihr Code hier\n",
    "# END Vorbelegung des Antwortfeldes\n",
    "    if n < 1:\n",
    "        raise ValueError(f\"Erwarte positiven Wert f√ºr n, ist aber {n}\")\n",
    "    result = 0.0\n",
    "    for i in range(n):\n",
    "        result += 1.0/(i+1)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Testfall 1: Fehlerbehandlung\n",
    "# Der Code in der Methode ist der Coderunner-Testfall\n",
    "#####################################################\n",
    "# Erwartete Ausgabe: VALUEERROR\n",
    "#####################################################\n",
    "\n",
    "# BEGIN Testfall 1\n",
    "try:\n",
    "    harmonic_series(-1)\n",
    "    print(\"NO ERROR\", end=\"\")\n",
    "except ValueError:\n",
    "    print(\"VALUEERROR\", end=\"\")\n",
    "# END Testfall 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Testfall 2: Fehlerbehandlung\n",
    "# Der Code in der Methode ist der Coderunner-Testfall\n",
    "#####################################################\n",
    "# Erwartete Ausgabe: VALUEERROR\n",
    "#####################################################\n",
    "\n",
    "# BEGIN Testfall 2\n",
    "try:\n",
    "    harmonic_series(-1)\n",
    "    print(\"NO ERROR\", end=\"\")\n",
    "except ValueError:\n",
    "    print(\"VALUEERROR\", end=\"\")\n",
    "# END Testfall 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Testfall 3: Fehlerbehandlung\n",
    "# Der Code in der Methode ist der Coderunner-Testfall\n",
    "#####################################################\n",
    "# Erwartete Ausgabe: VALUEERROR\n",
    "#####################################################\n",
    "\n",
    "# BEGIN Testfall 3\n",
    "import sys\n",
    "try:\n",
    "    harmonic_series(-(sys.maxsize-1))\n",
    "    print(\"NO ERROR\", end=\"\")\n",
    "except ValueError:\n",
    "    print(\"VALUEERROR\", end=\"\")\n",
    "# END Testfall 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Testfall 4: Funktion\n",
    "# Der Code in der Methode ist der Coderunner-Testfall\n",
    "#####################################################\n",
    "# Erwartete Ausgabe: OK\n",
    "#####################################################\n",
    "# BEGIN Testfall 4\n",
    "import math\n",
    "\n",
    "def __hsr(n:int):\n",
    "    if n < 1:\n",
    "        raise ValueError(f\"Erwarte positiven Wert f√ºr n, ist aber {n}\")\n",
    "    result = 0.0\n",
    "    for i in range(n):\n",
    "        result += 1.0/(i+1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def __xhr():\n",
    "    n=20000\n",
    "    for n in range(1, 20000,100):\n",
    "     rs = None\n",
    "     fr=__hsr(n) - harmonic_series(n)\n",
    "     if (math.fabs(fr) > 1E-10 ):\n",
    "            rs = (n, fr)\n",
    "            break;\n",
    "    if (rs is not None):\n",
    "        print(f\"NOT OK n={rs[0]}: {rs[1]}\", end=\"\")\n",
    "    else:\n",
    "        print(\"OK\", end=\"\")\n",
    "\n",
    "# Auskommentieren f√ºr Testfall        \n",
    "#__xhr()       \n",
    "\n",
    "# END Testfall 4\n",
    "    \n",
    "# Simulation Auswertung von Moodle-Coderunner-Testfall\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "    \n",
    "# Buffer f√ºr STDOUT\n",
    "buffer = io.StringIO()\n",
    "# STDOUT umleiten\n",
    "with redirect_stdout(buffer):\n",
    "   __xhr()\n",
    "        \n",
    "# Ausgabe abrufen\n",
    "output = buffer.getvalue()\n",
    "print(output)\n",
    "assert \"OK\" == output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dadae79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "3 != 0 : Should be 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m test\u001b[38;5;241m.\u001b[39mtest_sum()\n\u001b[1;32m     16\u001b[0m test\u001b[38;5;241m.\u001b[39mtest_sum_tuple()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_sum_no_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [12], line 11\u001b[0m, in \u001b[0;36mTestSum.test_sum_no_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_sum_no_values\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massertEqual\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mShould be 0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/unittest/case.py:873\u001b[0m, in \u001b[0;36mTestCase.assertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fail if the two objects are unequal as determined by the '=='\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m   operator.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m assertion_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getAssertEqualityFunc(first, second)\n\u001b[0;32m--> 873\u001b[0m \u001b[43massertion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/unittest/case.py:866\u001b[0m, in \u001b[0;36mTestCase._baseAssertEqual\u001b[0;34m(self, first, second, msg)\u001b[0m\n\u001b[1;32m    864\u001b[0m standardMsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m _common_shorten_repr(first, second)\n\u001b[1;32m    865\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_formatMessage(msg, standardMsg)\n\u001b[0;32m--> 866\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfailureException(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: 3 != 0 : Should be 0"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestSum(unittest.TestCase):\n",
    "    def test_sum(self):\n",
    "        self.assertEqual(6, 6, \"Should be 6\")\n",
    "\n",
    "    def test_sum_tuple(self):\n",
    "        self.assertEqual(5, 5, \"Should be 5\")\n",
    "\n",
    "    def test_sum_no_values(self):\n",
    "        self.assertEqual(3, 0, \"Should be 0\")\n",
    "\n",
    "\n",
    "test = TestSum()\n",
    "test.test_sum()\n",
    "test.test_sum_tuple()\n",
    "test.test_sum_no_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cae70",
   "metadata": {},
   "source": [
    "ChatGPT Vorschlag\n",
    "Der hier gezeigte Testcode wird im Gegensatz zu den \n",
    "\n",
    "# 1. Testfall nicht anzeigen: Template-Grader verwenden\n",
    "\n",
    "Wenn du Testcode nicht anzeigen willst, musst du den Testfall in den Template-Code verlegen, nicht in die expliziten ‚ÄûTestcases‚Äú.\n",
    "Damit wird im Fehlerfall kein einzelner Test angezeigt, sondern nur die Gesamtausgabe deines grading-Scripts.\n",
    "\n",
    "1. Fragetyp:<br>\n",
    "‚û§ ‚ÄûPython3: TemplateGrader‚Äú oder ‚ÄûCustom grading‚Äú verwenden.\n",
    "2. In den normalen Testf√§llen nur einen einzigen Dummy-Test lassen (oder gar keinen, je nach Fragetyp).\n",
    "3. Den kompletten Testcode in den Grading-Template verschieben:\n",
    "\n",
    "```\n",
    "# Der gesamte L√∂sungscode des Studenten ist hier bereits eingebettet ({{ STUDENT_ANSWER }})\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def run_tests():\n",
    "    results = []\n",
    "\n",
    "    # Beispieltest, dessen Code nicht angezeigt wird\n",
    "    try:\n",
    "        out = some_function(5)\n",
    "        if out == 42:\n",
    "            results.append({\"test\": \"T1\", \"result\": True})\n",
    "        else:\n",
    "            results.append({\"test\": \"T1\", \"result\": False, \"msg\": f\"Expected 42, got {out}\"})\n",
    "    except Exception as e:\n",
    "        results.append({\"test\": \"T1\", \"result\": False, \"msg\": str(e)})\n",
    "\n",
    "    return results\n",
    "\n",
    "print(json.dumps(run_tests()))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3fd7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "833ec6c8",
   "metadata": {},
   "source": [
    "So erreichst du trotzdem dein Ziel<br>\n",
    "(Testfall mit L√∂sungscode zum Wertevergleich verbergen, andere Testf√§lle als Hinweis anzeigen lassen.)\n",
    "\n",
    "üëâ L√∂sung: Alle Tests in den Template-Grader verlagern\n",
    "\n",
    "Du l√§sst die formerly ‚Äûnormalen‚Äú Tests weiterhin sichtbar erscheinen, aber ihre Bewertung l√§uft √ºber das JSON-Ergebnis des Template-Graders.\n",
    "\n",
    "So kannst du:\n",
    "\n",
    "‚úî jeden Test einzeln bewerten\n",
    "‚úî Punkte pro Test vergeben\n",
    "‚úî sichtbare Tests zeigen\n",
    "‚úî einen geheimen/komplexen Test enthalten\n",
    "‚úî alles zu einer Gesamtpunktzahl addieren\n",
    "‚úîÔ∏è Beispiel: Template-Grader mit gemischten Tests\n",
    "\n",
    "Angenommen:\n",
    "\n",
    "    Sichtbare Tests: 3 Tests √† 2 Punkte\n",
    "\n",
    "    Geheimer Test im Template-Grader: 4 Punkte\n",
    "\n",
    "Grading-Template (Python)\n",
    "\n",
    "```\n",
    "# STUDENT_ANSWER ist eingebettet\n",
    "{{ STUDENT_ANSWER }}\n",
    "\n",
    "import json\n",
    "\n",
    "results = []\n",
    "\n",
    "total_score = 0\n",
    "max_score = 0\n",
    "\n",
    "# --- Sichtbare Tests (entsprechen normalen Testcases) ---\n",
    "# Test 1\n",
    "max_score += 2\n",
    "try:\n",
    "    ok = (myfunc(1) == 2)\n",
    "    if ok:\n",
    "        total_score += 2\n",
    "    results.append({\"name\": \"Test 1\", \"passed\": ok})\n",
    "except Exception as e:\n",
    "    results.append({\"name\": \"Test 1\", \"passed\": False, \"error\": str(e)})\n",
    "\n",
    "# Test 2\n",
    "max_score += 2\n",
    "try:\n",
    "    ok = (myfunc(2) == 4)\n",
    "    if ok:\n",
    "        total_score += 2\n",
    "    results.append({\"name\": \"Test 2\", \"passed\": ok})\n",
    "except Exception as e:\n",
    "    results.append({\"name\": \"Test 2\", \"passed\": False, \"error\": str(e)})\n",
    "\n",
    "# Test 3\n",
    "max_score += 2\n",
    "try:\n",
    "    ok = (myfunc(3) == 6)\n",
    "    if ok:\n",
    "        total_score += 2\n",
    "    results.append({\"name\": \"Test 3\", \"passed\": ok})\n",
    "except Exception as e:\n",
    "    results.append({\"name\": \"Test 3\", \"passed\": False, \"error\": str(e)})\n",
    "\n",
    "# --- Geheimer Test ---\n",
    "max_score += 4\n",
    "try:\n",
    "    ok = (myfunc(10) == 20)\n",
    "    if ok:\n",
    "        total_score += 4\n",
    "    results.append({\"name\": \"Secret Test\", \"passed\": ok})\n",
    "except Exception as e:\n",
    "    results.append({\"name\": \"Secret Test\", \"passed\": False, \"error\": str(e)})\n",
    "\n",
    "# Gesamtergebnis zur√ºckgeben\n",
    "print(json.dumps({\n",
    "    \"testresults\": results,\n",
    "    \"score\": total_score / max_score   # CodeRunner erwartet einen Wert 0..1\n",
    "}))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596e92f",
   "metadata": {},
   "source": [
    "# GANZ Wichtig\n",
    "\n",
    "* Testf√§lle im Custom-Grader werden nicht im Test angezeigt.\n",
    "* Wenn ein Custom-Grader verwendet wird, muss dieser die Komplett-Bewertung √ºbernehmen.\n",
    "* Wenn Testf√§lle angezeigt werden sollen, m√ºssen sie mit dem Testfall-Editor zur Anzeige angelegt und zus√§tzlich im Custom-Grader zur Bewertung implementiert werden.\n",
    "\n",
    "ChatGPT:\n",
    "\n",
    "‚úîÔ∏è Grundregel (zentral wichtig)\n",
    "\n",
    "Sobald du einen Custom Template-Grader verwendest, ignoriert CodeRunner automatisch die normale Testfall-Bewertung.\n",
    "\n",
    "‚û°Ô∏è Die im Editor angelegten Tests werden ausgef√ºhrt, aber nicht bewertet.\n",
    "‚û°Ô∏è Nur der Custom-Grader bestimmt die Punkte.\n",
    "\n",
    "Daraus folgt:\n",
    "Wenn du die sichtbaren Tests mitbewerten willst, musst du sie im Custom-Grader selbst erneut auswerten.\n",
    "\n",
    "Das macht man normalerweise so:\n",
    "\n",
    "    Testcases im Editor anlegen ‚Üí damit sie f√ºr die Studierenden sichtbar sind.\n",
    "\n",
    "    Im Custom-Grader die gleichen Tests erneut programmatisch ausf√ºhren ‚Üí f√ºr die Punkte.\n",
    "\n",
    "Damit sind sie:\n",
    "\n",
    "    sichtbar, weil sie im Editor stehen,\n",
    "\n",
    "    bewertet, weil dein Grader sie ber√ºcksichtigt,\n",
    "\n",
    "    aber ohne Anzeige des Testcodes, weil die Bewertung durch dein Template-Grader erfolgt.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
